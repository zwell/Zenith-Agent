from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.chat_models import ChatTongyi
from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner
from langchain_community.agent_toolkits.playwright.toolkit import PlayWrightBrowserToolkit
from langchain_tavily import TavilySearch
from typing import Optional, Callable
from uuid import UUID
from langchain_core.callbacks import BaseCallbackHandler
from typing import Any, Dict, List, Optional, Callable, Union
from langchain_core.outputs import LLMResult, ChatGeneration
from config import settings
from src.tools.custom_tools import get_current_date
from src.tools.sanbox import SandboxToolManager
from src.llm_factory import get_llm

class StreamingCallbackHandler(BaseCallbackHandler):
    """ä¸€ä¸ªå¤„ç†æµå¼è¾“å‡ºçš„å›è°ƒå¤„ç†å™¨"""
    def __init__(self, send_event: Callable):
        self.send_event = send_event
        self.planner_run_id: Optional[UUID] = None

    async def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[Any]], **kwargs: Any
    ) -> None:
        """æ¨¡å‹å¼€å§‹ç”Ÿæˆæ—¶"""
        # await self.send_event("log", "æ€è€ƒä¸­...")
        pass

    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """æµå¼è·å– LLM çš„ token"""
        # å¯¹äºéæµå¼æ¨¡å‹ï¼Œè¿™ä¸ªå¯èƒ½ä¸ä¼šè¢«é¢‘ç¹è§¦å‘ï¼Œä½†å¯¹äºæµå¼æ¨¡å‹å¾ˆæœ‰ç”¨
        await self.send_event("log", f"LLM Token: {token}")

    async def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:
        """å·¥å…·å¼€å§‹æ‰§è¡Œæ—¶"""
        tool_name = serialized.get("name")
        # å°†æ—¥å¿—ä¿¡æ¯æ ¼å¼åŒ–å¾—æ›´æ˜“è¯»
        await self.send_event("log", f"â³ æ­£åœ¨è°ƒç”¨å·¥å…·: `{tool_name}` | è¾“å…¥: `{input_str}`")

    async def on_tool_end(self, output: str, **kwargs: Any) -> None:
        """å·¥å…·æ‰§è¡Œç»“æŸæ—¶"""
        if output is not None:
            await self.send_event("log", f"âœ… å·¥å…·è¿”å›: `{output[:200]}...`")

    async def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], *, run_id: UUID, parent_run_id: Optional[UUID] = None, **kwargs: Any
    ) -> None:
        print(parent_run_id, run_id)
        print("on_chain_start", inputs)
        # --- å…³é”®é€»è¾‘ï¼šè¯†åˆ« Planner Chain ---
        # PlanAndExecute Agentå†…éƒ¨çš„Planner Chainé€šå¸¸æœ‰ä¸€ä¸ªç‰¹å®šçš„åç§°æˆ–ç»“æ„
        # æˆ‘ä»¬å¯ä»¥é€šè¿‡ `serialized.get('name')` æˆ–å…¶ä»–ç‰¹å¾æ¥è¯†åˆ«å®ƒ
        # ä¸€ä¸ªç®€å•çš„ï¼ˆä½†ä¸å®Œå…¨å¯é çš„ï¼‰æ–¹æ³•æ˜¯å‡è®¾ç¬¬ä¸€ä¸ªéæœ€å¤–å±‚çš„é“¾æ˜¯Planner
        # æ›´å¯é çš„æ–¹æ³•æ˜¯æ£€æŸ¥ 'id' æˆ– 'name' å­—æ®µ
        # ä¾‹å¦‚ï¼Œload_chat_planner åˆ›å»ºçš„é“¾çš„idå¯èƒ½åŒ…å« "LLMChain"
        if self.planner_run_id is None and parent_run_id is not None:
             # å‡è®¾ç¬¬ä¸€ä¸ªå­é“¾æ˜¯ planner
             self.planner_run_id = run_id
             await self.send_event("log", "ğŸ“ è§„åˆ’å™¨å·²å¯åŠ¨ï¼Œæ­£åœ¨åˆ¶å®šè®¡åˆ’...")

    async def on_chain_end(
        self, outputs: Dict[str, Any], *, run_id: UUID, **kwargs: Any
    ) -> Any:
        print("on_chain_end", outputs, run_id)
        """åœ¨é“¾ç»“æŸæ—¶è§¦å‘"""
        # --- å…³é”®é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ Planner Chain ç»“æŸ ---
        if run_id == self.planner_run_id:
            # å¦‚æœæ˜¯ Planner Chain ç»“æŸï¼Œå°±æå–è®¡åˆ’å¹¶å‘é€
            plan_text = outputs.get('text', 'æ— æ³•æå–è®¡åˆ’æ–‡æœ¬ã€‚')
            await self.send_event("plan", plan_text)
            self.planner_run_id = None # é‡ç½® run_idï¼Œä»¥å¤‡å°†æ¥ä½¿ç”¨

async def create_agent(browser, sandbox, stream_callback: Optional[Callable] = None):
    """
    æ ¹æ®ä¼ å…¥çš„æµè§ˆå™¨å’Œæ²™ç®±å®ä¾‹ï¼Œåˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªPlan-and-Execute Agentã€‚
    """

    # è§„åˆ’å™¨
    plan_llm = get_llm(
        provider=settings.PLANNER_LLM_PROVIDER,
        model_name=settings.PLANNER_LLM_MODEL,
        temperature=settings.PLANNER_LLM_TEMPERATURE
    )
    planner = load_chat_planner(
        plan_llm, 
        system_prompt=settings.PLANNER_PROMPT,
    )

    # æ‰§è¡Œå™¨
    executor_llm = get_llm(
        provider=settings.EXECUTOR_LLM_PROVIDER,
        model_name=settings.EXECUTOR_LLM_MODEL,
        temperature=settings.EXECUTOR_LLM_TEMPERATURE
    )

    # å·¥å…·é›†
    tools = [get_current_date]
    tools.extend(PlayWrightBrowserToolkit.from_browser(async_browser=browser).get_tools())
    tools.append(TavilySearch(max_results=3)) 

    # E2Bæ²™ç®±å·¥å…·
    sandbox_tool_manager = SandboxToolManager(sandbox)
    tools.extend(sandbox_tool_manager.get_all_tools())
    
    # verbose=True ä¼šåœ¨æ‰§è¡Œæ—¶æ‰“å°è¯¦ç»†æ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯•
    executor = load_agent_executor(
        executor_llm, 
        tools, 
        verbose=True,
    )

    return PlanAndExecute(
        planner=planner, 
        executor=executor, 
        verbose=True,
        callbacks=[StreamingCallbackHandler(stream_callback)]
    )